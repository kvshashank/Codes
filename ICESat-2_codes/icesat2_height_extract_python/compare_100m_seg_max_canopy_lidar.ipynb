{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe2e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the atl08 segment file\n",
    "## For each line,\n",
    "## Extract date and select the corresponding ATL03 h5 file\n",
    "## Use the seg id begin and end info to extract lat and lon of all photons that lie within those 20m segments\n",
    "## Create a df with columns [\"100m_seg_index\", \"ph_lat\", \"ph_lon\", \"quality\"] and keep appending\n",
    "\n",
    "## Convert each lat lon to northing and easting.\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5091b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pyproj\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37395111",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract parameters from the ATL08 product first. This contains several flags\n",
    "### such as night flag, cloud flag, canopy uncertainty, watermask etc.\n",
    "\n",
    "## Create a dataframe for collecting all data\n",
    "df = pd.DataFrame([], columns=[\"date\", \"lats\", \"lons\", \"seg_id_begin\", \"seg_id_end\", \"night_flag\", \n",
    "                              \"canopy_openness\", \"h_canopy\", \"canopy_flag\", \"h_canopy_uncertainty\", \n",
    "                               \"cloud_flag_atm\", \"water_mask\", \"landcover\", \"beam_num\"])\n",
    "\n",
    "### Extract data for each hdf5 file in the folder from Oct 2018 to Feb 2021\n",
    "\n",
    "os.chdir('/home/shashank/Downloads/icesat2/icesat2_data/ATL08/')\n",
    "\n",
    "for filename in sorted(glob.glob(\"*.h5\")):    \n",
    "    \n",
    "     #### Extract date from file name    \n",
    "     date_str = filename.split(\"_\")[1][0:8]\n",
    "     date_obj  = datetime.strptime(date_str,'%Y%m%d')\n",
    "     date = datetime.strftime(date_obj, '%d %B,%Y')\n",
    "     \n",
    "     ### Import the hdf file \n",
    "     f = h5py.File(filename, 'r')\n",
    "\n",
    "     ### Extract data for all 3 strong beams\n",
    "     beam_numbers = ['/gt1r', '/gt2r', '/gt3r']\n",
    "\n",
    "     for beam in beam_numbers: \n",
    "         ### Extract required parameters from hdf\n",
    "         ### Each of these parameters are calculated for a 100m segment\n",
    "         lons = pd.DataFrame(f[beam + '/land_segments/longitude'][:])\n",
    "         lats = pd.DataFrame(f[beam + '/land_segments/latitude'][:])\n",
    "         segment_id_begin = pd.DataFrame(f[beam + '/land_segments/segment_id_beg'][:])\n",
    "         segment_id_end = pd.DataFrame(f[beam + '/land_segments/segment_id_end'][:])\n",
    "         night_flag = pd.DataFrame(f[beam + '/land_segments/night_flag'][:]) ## day=0, night=1 \n",
    "         can_open = pd.DataFrame(f[beam + '/land_segments/canopy/canopy_openness'][:]) ### ** ###\n",
    "         h_canopy = pd.DataFrame(f[beam + '/land_segments/canopy/h_canopy'][:])\n",
    "         h_can_uncertainty = pd.DataFrame(f[beam + '/land_segments/canopy/h_canopy_uncertainty'][:])\n",
    "         cloud_flag_atm = pd.DataFrame(f[beam + '/land_segments/cloud_flag_atm'][:]) ## Flag > 0: Clouds/aerosols could be present\n",
    "         seg_watermask = pd.DataFrame(f[beam + '/land_segments/segment_watermask'][:]) ## 0:no water 1:water\n",
    "         seg_landcover = pd.DataFrame(f[beam + '/land_segments/segment_landcover'][:]) ## MODIS landcover 17 classes http://www.worldhairsalon.com/sage/data-and-models/friedl_rse2010.pdf   \n",
    "   \n",
    "         ## Add a column with strong beam number\n",
    "         beam_num = []\n",
    "         for i in range(0, len(lons)):\n",
    "            beam_num.append(beam)\n",
    "         beam_num = pd.DataFrame(beam_num, columns=[\"\"])\n",
    "    \n",
    "         ## Create a variable which counts the number of times '1' appears in the canopy flag\n",
    "         ## -1: no data within geosegment available for analysis; 0: indicates no canopy photons within geosegment; 1: indicates canopy photons within geosegment\n",
    "         ## For example, a 100 m ATL08 segment might have the following subset_can_flags: {-1 -1 -1 1 1} which would translate that no photons (canopy or ground) were available for processing in the first three geosegments.\n",
    "         canopy_flag = pd.DataFrame(f[beam + '/land_segments/canopy/subset_can_flag'][:])\n",
    "         counter = pd.DataFrame([], index=np.arange(0, len(canopy_flag)), columns=[\"counter\"])\n",
    "         for i in range(0, len(counter)):\n",
    "            count=0\n",
    "            segment = canopy_flag.iloc[i,:]\n",
    "            for j in range(0, 5):\n",
    "                if segment[j] == 1:\n",
    "                  count+=1\n",
    "            counter[\"counter\"][i] = count         \n",
    "            \n",
    "         ### Create a column of dates \n",
    "         dates_arr = []\n",
    "         for i in range(0, len(lats)):\n",
    "            dates_arr.append(date)\n",
    "            dates_df = pd.DataFrame(dates_arr, columns=[\"date\"])\n",
    "         \n",
    "         ### Combine all the extracted parameters into a dataframe\n",
    "         canopy_height_data = pd.concat([dates_df, lats, lons, segment_id_begin, segment_id_end, night_flag, \n",
    "                                       can_open, h_canopy, counter, h_can_uncertainty, cloud_flag_atm, \n",
    "                                        seg_watermask, seg_landcover, beam_num], axis=1, ignore_index=True)\n",
    "         canopy_height_data.columns = df.columns\n",
    "            \n",
    "         ### Drop pixels with no-data value and canopy openness value equal to 3.4028235e+38\n",
    "         ## Consider only pixels with h_canopy_uncertainty < 10\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"canopy_openness\"] < 1e+38]\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"h_canopy\"] < 1e+38]\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"h_canopy_uncertainty\"] < 5]\n",
    "        \n",
    "         ### Consider only pixels within the Cabo Rojo region\n",
    "         canopy_height_data = canopy_height_data[(canopy_height_data[\"lats\"]>18) & \n",
    "                                                 (canopy_height_data[\"lats\"]<18.2)]\n",
    "         canopy_height_data = canopy_height_data[(canopy_height_data[\"lons\"] > -67.20) &\n",
    "                                                 (canopy_height_data[\"lons\"] < -66.78)]\n",
    "            \n",
    "         ### Extract places with no cloud/aerosol cover and no water\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"cloud_flag_atm\"] == 0]\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"water_mask\"] == 0]   \n",
    "    \n",
    "         ### Extract only day/night time data\n",
    "         #canopy_height_data = canopy_height_data[canopy_height_data[\"night_flag\"]==1]\n",
    "        \n",
    "         ## Remove unwanted landcover classes permanent wetlands(11), urban(13), barren(16), water(17)\n",
    "         #canopy_height_data = canopy_height_data[(canopy_height_data[\"landcover\"]!=13) &\n",
    "         #                                        (canopy_height_data[\"landcover\"]<=14) &\n",
    "         #                                        (canopy_height_data[\"landcover\"]!=11)]\n",
    "        \n",
    "         ### Extract data with data quality counter\n",
    "         ## Most points have counter value 5. So, choosing only those. \n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"canopy_flag\"]==5]\n",
    "            \n",
    "         ### Append canopy height data at the bottom of df\n",
    "         df = df.append(canopy_height_data, ignore_index=True)\n",
    "\n",
    "df.to_csv(\"ATL08_shortlisted_segments.csv\", sep=\",\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26ab09ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650464, 3)\n",
      "               lats       lons  quality\n",
      "26209073  18.000010 -66.838687        0\n",
      "26209074  18.000007 -66.838686        0\n",
      "26209075  18.000003 -66.838685        0\n",
      "26209077  18.000017 -66.838688        0\n",
      "26209078  18.000002 -66.838682        0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/shashank/Downloads/icesat2/icesat2_data/ATL03/sample_h5_cabo_rojo/\")\n",
    "f = h5py.File(\"ATL03_20181028191317_04610101_004_01.h5\", 'r')\n",
    "ph_lats = pd.DataFrame(f[\"gt1r/heights/lat_ph\"][:])\n",
    "ph_lons = pd.DataFrame(f[\"gt1r/heights/lon_ph\"][:])\n",
    "quality = pd.DataFrame(f['gt1r/heights/quality_ph'][:])\n",
    "photon_level_data = pd.concat([ph_lats, ph_lons, quality], axis=1)\n",
    "photon_level_data.columns = [\"lats\", \"lons\", \"quality\"]\n",
    "photon_level_data = photon_level_data[(photon_level_data[\"lats\"]>18) & \n",
    "                                                 (photon_level_data[\"lats\"]<18.2)]\n",
    "photon_level_data = photon_level_data[(photon_level_data[\"lons\"] > -67.20) &\n",
    "                                                 (photon_level_data[\"lons\"] < -66.78)]\n",
    "photon_level_data = photon_level_data[(photon_level_data[\"quality\"] == 0)]\n",
    "print(photon_level_data.shape)\n",
    "print(photon_level_data.head())\n",
    "#ph_index_begin = pd.DataFrame(f['gt1r/geolocation/ph_index_beg'][:])\n",
    "#segment_id = pd.DataFrame(f['gt1r/geolocation/segment_id'][:])\n",
    "#print(len(segment_id), len(ph_index_begin))\n",
    "#print(len(ph_lats), len(ph_lons), len(ph_id_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76725e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/shashank/Downloads/icesat2/icesat2_data/ATL08/\")\n",
    "pd.read_csv(\"ATL08_shortlisted_segments.csv\", header=0)\n",
    "\n",
    "seg_ids_20m = []\n",
    "      for seg_id in range(shortlisted_seg[\"seg_id_begin\"][line], (shortlisted_seg[\"seg_id_end\"][line] + 1)):\n",
    "            seg_ids_20m.append(seg_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
