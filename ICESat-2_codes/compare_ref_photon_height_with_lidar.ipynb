{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedbc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pyproj\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86106d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract parameters from the ATL08 product first. This contains several flags\n",
    "### such as night flag, cloud flag, canopy uncertainty, watermask etc.\n",
    "\n",
    "## Create a dataframe for collecting all data\n",
    "df = pd.DataFrame([], columns=[\"date\", \"lats\", \"lons\", \"seg_id_begin\", \"seg_id_end\", \"night_flag\", \n",
    "                              \"canopy_openness\", \"canopy_flag\", \"h_canopy_uncertainty\", \"cloud_flag_atm\", \n",
    "                              \"water_mask\", \"landcover\"])\n",
    "\n",
    "### Extract data for each hdf5 file in the folder from Oct 2018 to Feb 2021\n",
    "\n",
    "os.chdir('/home/shashank/Downloads/icesat2/icesat2_data/ATL08/')\n",
    "\n",
    "for filename in sorted(glob.glob(\"*.h5\")):    \n",
    "    \n",
    "     #### Extract date from file name    \n",
    "     date_str = filename.split(\"_\")[1][0:8]\n",
    "     date_obj  = datetime.strptime(date_str,'%Y%m%d')\n",
    "     date = datetime.strftime(date_obj, '%d %B,%Y')\n",
    "     \n",
    "     ### Import the hdf file \n",
    "     f = h5py.File(filename, 'r')\n",
    "\n",
    "     ### Extract data for all 3 strong beams\n",
    "     beam_numbers = ['/gt1r', '/gt2r', '/gt3r']\n",
    "\n",
    "     for beam in beam_numbers: \n",
    "         ### Extract required parameters from hdf\n",
    "         ### Each of these parameters are calculated for a 100m segment\n",
    "         lons = pd.DataFrame(f[beam + '/land_segments/longitude'][:])\n",
    "         lats = pd.DataFrame(f[beam + '/land_segments/latitude'][:])\n",
    "         segment_id_begin = pd.DataFrame(f[beam + '/land_segments/segment_id_beg'][:])\n",
    "         segment_id_end = pd.DataFrame(f[beam + '/land_segments/segment_id_end'][:])\n",
    "         night_flag = pd.DataFrame(f[beam + '/land_segments/night_flag'][:]) ## day=0, night=1 \n",
    "         can_open = pd.DataFrame(f[beam + '/land_segments/canopy/canopy_openness'][:]) ### ** ###\n",
    "         h_can_uncertainty = pd.DataFrame(f[beam + '/land_segments/canopy/h_canopy_uncertainty'][:])\n",
    "         cloud_flag_atm = pd.DataFrame(f[beam + '/land_segments/cloud_flag_atm'][:]) ## Flag > 0: Clouds/aerosols could be present\n",
    "         seg_watermask = pd.DataFrame(f[beam + '/land_segments/segment_watermask'][:]) ## 0:no water 1:water\n",
    "         seg_landcover = pd.DataFrame(f[beam + '/land_segments/segment_landcover'][:]) ## MODIS landcover 17 classes http://www.worldhairsalon.com/sage/data-and-models/friedl_rse2010.pdf   \n",
    "   \n",
    "         ## Create a variable which counts the number of times '1' appears in the canopy flag\n",
    "         ## -1: no data within geosegment available for analysis; 0: indicates no canopy photons within geosegment; 1: indicates canopy photons within geosegment\n",
    "         ## For example, a 100 m ATL08 segment might have the following subset_can_flags: {-1 -1 -1 1 1} which would translate that no photons (canopy or ground) were available for processing in the first three geosegments.\n",
    "         canopy_flag = pd.DataFrame(f[beam + '/land_segments/canopy/subset_can_flag'][:])\n",
    "         counter = pd.DataFrame([], index=np.arange(0, len(canopy_flag)), columns=[\"counter\"])\n",
    "         for i in range(0, len(counter)):\n",
    "            count=0\n",
    "            segment = canopy_flag.iloc[i,:]\n",
    "            for j in range(0, 5):\n",
    "                if segment[j] == 1:\n",
    "                  count+=1\n",
    "            counter[\"counter\"][i] = count         \n",
    "            \n",
    "         ### Create a column of dates \n",
    "         dates_arr = []\n",
    "         for i in range(0, len(lats)):\n",
    "            dates_arr.append(date)\n",
    "            dates_df = pd.DataFrame(dates_arr, columns=[\"date\"])\n",
    "         \n",
    "         ### Combine all the extracted parameters into a dataframe\n",
    "         canopy_height_data = pd.concat([dates_df, lats, lons, segment_id_begin, segment_id_end, night_flag, \n",
    "                                       can_open, counter, h_can_uncertainty, cloud_flag_atm, \n",
    "                                        seg_watermask, seg_landcover], axis=1, ignore_index=True)\n",
    "         canopy_height_data.columns = df.columns\n",
    "            \n",
    "         ### Drop pixels with no-data value and canopy openness value equal to 3.4028235e+38\n",
    "         ## Consider only pixels with h_canopy_uncertainty < 10\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"canopy_openness\"] < 1e+38]\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"h_canopy_uncertainty\"] < 5]\n",
    "        \n",
    "         ### Consider only pixels within the Cabo Rojo region\n",
    "         canopy_height_data = canopy_height_data[(canopy_height_data[\"lats\"]>18) & \n",
    "                                                 (canopy_height_data[\"lats\"]<18.2)]\n",
    "         canopy_height_data = canopy_height_data[(canopy_height_data[\"lons\"] > -67.20) &\n",
    "                                                 (canopy_height_data[\"lons\"] < -66.78)]\n",
    "            \n",
    "         ### Extract places with no cloud/aerosol cover and no water\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"cloud_flag_atm\"] == 0]\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"water_mask\"] == 0]   \n",
    "    \n",
    "         ### Extract only day/night time data\n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"night_flag\"]==1]\n",
    "        \n",
    "         ## Remove unwanted landcover classes permanent wetlands(11), urban(13), barren(16), water(17)\n",
    "         #canopy_height_data = canopy_height_data[(canopy_height_data[\"landcover\"]!=13) &\n",
    "         #                                        (canopy_height_data[\"landcover\"]<=14) &\n",
    "         #                                        (canopy_height_data[\"landcover\"]!=11)]\n",
    "        \n",
    "         ### Extract data with data quality counter\n",
    "         ## Most points have counter value 5. So, choosing only those. \n",
    "         canopy_height_data = canopy_height_data[canopy_height_data[\"canopy_flag\"]==5]\n",
    "            \n",
    "         ### Append canopy height data at the bottom of df\n",
    "         df = df.append(canopy_height_data, ignore_index=True)\n",
    "\n",
    "df.to_csv(\"ATL08_shortlisted_segments.csv\", sep=\",\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75340214",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/shashank/Downloads/icesat2/icesat2_data/ATL08\")\n",
    "atl08 = pd.read_csv(\"ATL08_shortlisted_segments.csv\", header=0)\n",
    "atl08 = atl08[atl08[\"h_canopy_uncertainty\"]<5].reset_index(drop=True)\n",
    "\n",
    "## Create a list of 20-m segment ids \n",
    "seg_20m_list = []\n",
    "for i in range(0, len(atl08)):\n",
    "    for j in range(int(atl08[\"seg_id_begin\"][i]), int(atl08[\"seg_id_end\"][i] + 1)):\n",
    "        seg_20m_list.append(j)\n",
    "\n",
    "## Then extract the ATL03 parameters. This code considers only one h5 file. Complete analysis on Theseus. \n",
    "## The Theseus code is present at the location /home/vk5/icesat_2\n",
    "\n",
    "os.chdir('/home/shashank/Downloads/icesat2/icesat2_data/ATL03/sample_h5_cabo_rojo')\n",
    "f = h5py.File('ATL03_20181028191317_04610101_004_01.h5', 'r')\n",
    "atl03_photon_data = pd.DataFrame([], columns=[\"reference_ph_lat\", \"reference_ph_lon\", \"segment_id\", \n",
    "                                              \"index_within_seg\"])\n",
    "\n",
    "beam_numbers = ['/gt1r', '/gt2r', '/gt3r']\n",
    "\n",
    "for beam in beam_numbers:\n",
    "    #ph_lats = pd.DataFrame(f[beam + '/heights/lat_ph'][:])\n",
    "    #ph_lons = pd.DataFrame(f[beam + '/heights/lon_ph'][:])\n",
    "    #id_count = pd.DataFrame(f[beam + '/heights/ph_id_count'][:])\n",
    "    #quality = pd.DataFrame(f[beam + '/heights/quality_ph'][:]) ## value >= 1 : possible afterpulse\n",
    "    ref_ph_index_within_seg = pd.DataFrame(f[beam + '/geolocation/reference_photon_index'][:], columns=[\"\"])\n",
    "    ref_ph_lat = pd.DataFrame(f[beam + '/geolocation/reference_photon_lat'][:])\n",
    "    ref_ph_lon = pd.DataFrame(f[beam + '/geolocation/reference_photon_lon'][:])\n",
    "    seg_id = pd.DataFrame(f[beam + '/geolocation/segment_id'][:])\n",
    "    \n",
    "    ## Combine reference photon information\n",
    "    ref_ph = pd.concat([ref_ph_lat, ref_ph_lon, seg_id, ref_ph_index_within_seg], axis=1)\n",
    "    ref_ph.columns = atl03_photon_data.columns\n",
    "    \n",
    "    ## Extract only those 20-m segment ids which exist in atl08 shortlist \n",
    "    ref_ph = ref_ph[ref_ph[\"segment_id\"].isin(seg_20m_list)].reset_index(drop=True)\n",
    "    \n",
    "    atl03_photon_data = atl03_photon_data.append(ref_ph, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bac180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  segment_id ph_index  ph_height\n",
      "0         11      746  76.948997\n",
      "1         12      442  75.489204\n",
      "2         12      526  75.669113\n",
      "3         12      654  75.663261\n",
      "4         12      822  73.567245\n"
     ]
    }
   ],
   "source": [
    "### Extract canopy heights from ATL08 photon data. \n",
    "os.chdir('/home/shashank/Downloads/icesat2/icesat2_data/ATL08')\n",
    "f = h5py.File('ATL08_20181028191317_04610101_004_01.h5', 'r')\n",
    "\n",
    "atl08_photon_data = pd.DataFrame([], columns=[\"segment_id\", \"ph_index\",  \n",
    "                                              \"ph_height\"])\n",
    "\n",
    "for beam in beam_numbers:\n",
    "   classed_PC_indx = pd.DataFrame(f[beam + '/signal_photons/classed_pc_indx'][:])\n",
    "   ph_segment_id = pd.DataFrame(f[beam + '/signal_photons/ph_segment_id'][:])\n",
    "   photon_class = pd.DataFrame(f[beam + '/signal_photons/classed_pc_flag'][:]) ## 0: noise, 1: ground, 2:canopy, 3: TOC\n",
    "   ph_h = pd.DataFrame(f[beam + '/signal_photons/ph_h'][:])\n",
    "   d_flag = pd.DataFrame(f[beam + '/signal_photons/d_flag'][:]) ## 0:noise 1:signal\n",
    "   \n",
    "   photon_df = pd.concat([ph_segment_id, classed_PC_indx, ph_h, photon_class,  d_flag], axis=1)\n",
    "   photon_df.columns = [\"segment_id\", \"ph_index\", \"ph_height\", \"photon_class\", \"d_flag\"]\n",
    "   photon_df = photon_df[photon_df[\"d_flag\"]==1].reset_index(drop=True)\n",
    "   photon_df = photon_df[(photon_df[\"photon_class\"] == 2) | (photon_df[\"photon_class\"] == 3)].reset_index(drop=True)\n",
    "   photon_df = photon_df.drop(columns=[\"photon_class\", \"d_flag\"], axis=1)\n",
    "   atl08_photon_data = atl08_photon_data.append(photon_df)\n",
    "print(atl08_photon_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46edcd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18596, 3)\n"
     ]
    }
   ],
   "source": [
    "common_seg_ids = np.intersect1d(atl08_photon_data[\"segment_id\"], atl03_photon_data[\"segment_id\"])\n",
    "atl03_common_seg = atl03_photon_data[atl03_photon_data[\"segment_id\"].isin(common_seg_ids)].reset_index(drop=True)\n",
    "atl08_common_seg = atl08_photon_data[atl08_photon_data[\"segment_id\"].isin(common_seg_ids)].reset_index(drop=True)\n",
    "print(atl08_common_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "849d6124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3)\n",
      "   reference_ph_lat  reference_ph_lon  ph_height\n",
      "0         17.997445        -66.838402   3.303221\n",
      "1         17.997807        -66.838442   4.151279\n",
      "2         17.997984        -66.838461   2.479109\n",
      "3         18.019217        -66.810438  20.451130\n",
      "4         18.019396        -66.810457  19.704277\n"
     ]
    }
   ],
   "source": [
    "ref_ph_lat_lon_h = pd.DataFrame([], columns=[\"reference_ph_lat\", \"reference_ph_lon\", \"ph_height\"])\n",
    "\n",
    "for seg_id in common_seg_ids:\n",
    "    atl03_sub = atl03_common_seg[atl03_common_seg[\"segment_id\"] == seg_id].reset_index(drop=True)\n",
    "    atl08_sub = atl08_common_seg[atl08_common_seg[\"segment_id\"] == seg_id].reset_index(drop=True)\n",
    "    if len(np.intersect1d(atl03_sub[\"index_within_seg\"], atl08_sub[\"ph_index\"])) > 0:\n",
    "       for i in np.intersect1d(atl03_sub[\"index_within_seg\"], atl08_sub[\"ph_index\"]):\n",
    "          lat = atl03_sub[atl03_sub[\"index_within_seg\"] == i][\"reference_ph_lat\"].reset_index(drop=True)\n",
    "          lon = atl03_sub[atl03_sub[\"index_within_seg\"] == i][\"reference_ph_lon\"].reset_index(drop=True)\n",
    "          height = atl08_sub[atl08_sub[\"ph_index\"] == i][\"ph_height\"].reset_index(drop=True)\n",
    "          photon_info = pd.concat([lat, lon, height], axis=1)\n",
    "          ref_ph_lat_lon_h = ref_ph_lat_lon_h.append(photon_info, ignore_index=True)\n",
    "    \n",
    "print(ref_ph_lat_lon_h.shape)\n",
    "print(ref_ph_lat_lon_h.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7c37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
